{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "from keras.layers import Dense,Dropout,Activation,Conv2D,MaxPooling2D,BatchNormalization,Flatten\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from keras.callbacks import EarlyStopping,ReduceLROnPlateau,ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "import cv2\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "from keras.src.utils.np_utils import to_categorical\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".DS_Store\n",
      "test\n",
      "happy\n",
      ".DS_Store\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "neutral\n",
      "angry\n",
      "train\n",
      "happy\n",
      ".DS_Store\n",
      "sad\n",
      "fear\n",
      "surprise\n",
      "neutral\n",
      "angry\n"
     ]
    }
   ],
   "source": [
    "int2emotions = {0:'angry',1:'fear',2:'happy',3:'neutral',4:'sad',5:'surprise'}\n",
    "emotions2int = {'angry':0,'fear':1,'happy':2,'neutral':3,'sad':4,'surprise':5}\n",
    "\n",
    "dic = {'images':[], 'labels':[], 'purpose':[]}\n",
    "    \n",
    "for d in os.listdir('Data/'):\n",
    "    print(d)\n",
    "    if(d!='.DS_Store'):\n",
    "        for emotion in os.listdir(f'Data/{d}'):\n",
    "            print(emotion)\n",
    "            if(emotion!='.DS_Store'):\n",
    "                for i in os.listdir(f'Data/{d}/{emotion}'):\n",
    "                    img = cv2.imread(f'Data/{d}/{emotion}/{i}',0)\n",
    "                    img = img.reshape(48,48,1)\n",
    "            \n",
    "                    dic['images'].append(img)\n",
    "                    dic['labels'].append(emotion)\n",
    "            \n",
    "                    if d=='train':\n",
    "                        dic['purpose'].append('T')\n",
    "                    else:\n",
    "                        dic['purpose'].append('V')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[5], [4], [5], [9], [10], [9], [10], [12], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[19], [21], [22], [18], [20], [21], [16], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[228], [229], [230], [229], [228], [227], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[25], [33], [43], [30], [46], [84], [105], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[33], [29], [15], [15], [20], [36], [40], [5...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[5], [4], [5], [9], [10], [9], [10], [12], [...  happy       V\n",
       "1  [[[19], [21], [22], [18], [20], [21], [16], [1...  happy       V\n",
       "2  [[[228], [229], [230], [229], [228], [227], [2...  happy       V\n",
       "3  [[[25], [33], [43], [30], [46], [84], [105], [...  happy       V\n",
       "4  [[[33], [29], [15], [15], [20], [36], [40], [5...  happy       V"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.DataFrame(dic)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "train_data = df[df['purpose']=='T']\n",
    "val_data = df[df['purpose']=='V']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>7067</th>\n",
       "      <td>[[[108], [83], [63], [65], [89], [111], [121],...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7068</th>\n",
       "      <td>[[[137], [142], [159], [162], [158], [134], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7069</th>\n",
       "      <td>[[[111], [148], [155], [167], [181], [191], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7070</th>\n",
       "      <td>[[[151], [156], [121], [100], [80], [116], [15...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7071</th>\n",
       "      <td>[[[248], [187], [149], [130], [97], [140], [13...</td>\n",
       "      <td>happy</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 images labels purpose\n",
       "7067  [[[108], [83], [63], [65], [89], [111], [121],...  happy       T\n",
       "7068  [[[137], [142], [159], [162], [158], [134], [1...  happy       T\n",
       "7069  [[[111], [148], [155], [167], [181], [191], [1...  happy       T\n",
       "7070  [[[151], [156], [121], [100], [80], [116], [15...  happy       T\n",
       "7071  [[[248], [187], [149], [130], [97], [140], [13...  happy       T"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[5], [4], [5], [9], [10], [9], [10], [12], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[19], [21], [22], [18], [20], [21], [16], [1...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[228], [229], [230], [229], [228], [227], [2...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[25], [33], [43], [30], [46], [84], [105], [...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[33], [29], [15], [15], [20], [36], [40], [5...</td>\n",
       "      <td>happy</td>\n",
       "      <td>V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images labels purpose\n",
       "0  [[[5], [4], [5], [9], [10], [9], [10], [12], [...  happy       V\n",
       "1  [[[19], [21], [22], [18], [20], [21], [16], [1...  happy       V\n",
       "2  [[[228], [229], [230], [229], [228], [227], [2...  happy       V\n",
       "3  [[[25], [33], [43], [30], [46], [84], [105], [...  happy       V\n",
       "4  [[[33], [29], [15], [15], [20], [36], [40], [5...  happy       V"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "happy       7215\n",
       "neutral     4965\n",
       "sad         4830\n",
       "fear        4097\n",
       "angry       3995\n",
       "surprise    3171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "happy       1774\n",
       "sad         1247\n",
       "neutral     1233\n",
       "fear        1024\n",
       "angry        958\n",
       "surprise     831\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>images</th>\n",
       "      <th>labels</th>\n",
       "      <th>purpose</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[[[83], [71], [64], [67], [74], [83], [85], [9...</td>\n",
       "      <td>sad</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[[[58], [60], [57], [59], [19], [1], [8], [2],...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[[[255], [255], [255], [255], [255], [255], [2...</td>\n",
       "      <td>surprise</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[[[255], [255], [255], [255], [255], [255], [2...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[[[193], [196], [191], [190], [193], [191], [1...</td>\n",
       "      <td>angry</td>\n",
       "      <td>T</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              images    labels purpose\n",
       "0  [[[83], [71], [64], [67], [74], [83], [85], [9...       sad       T\n",
       "1  [[[58], [60], [57], [59], [19], [1], [8], [2],...  surprise       T\n",
       "2  [[[255], [255], [255], [255], [255], [255], [2...  surprise       T\n",
       "3  [[[255], [255], [255], [255], [255], [255], [2...   neutral       T\n",
       "4  [[[193], [196], [191], [190], [193], [191], [1...     angry       T"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "happy_df = train_data[train_data['labels']=='happy'].sample(n=3171)\n",
    "neutral_df = train_data[train_data['labels']=='neutral'].sample(n=3171)\n",
    "sad_df = train_data[train_data['labels']=='sad'].sample(n=3171)\n",
    "fear_df = train_data[train_data['labels']=='fear'].sample(n=3171)\n",
    "angry_df = train_data[train_data['labels']=='angry'].sample(n=3171)\n",
    "surprise_df = train_data[train_data['labels']=='surprise'].sample(n=3171)\n",
    "\n",
    "train_data = pd.concat([happy_df,neutral_df,sad_df,fear_df,angry_df,surprise_df])\n",
    "\n",
    "train_data = train_data.sample(frac=1)\n",
    "train_data.reset_index(inplace=True)\n",
    "train_data.drop('index',inplace=True,axis=1)\n",
    "\n",
    "train_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "labels\n",
       "sad         3171\n",
       "surprise    3171\n",
       "neutral     3171\n",
       "angry       3171\n",
       "happy       3171\n",
       "fear        3171\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['labels'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Axes: xlabel='count', ylabel='labels'>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "sns.countplot(train_data['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size= 32\n",
    "classes = 6\n",
    "rows,columns=48,48"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_labels = list(train_data['labels'].replace(emotions2int))\n",
    "train_labels = to_categorical(train_labels)\n",
    "\n",
    "val_labels = list(val_data['labels'].replace(emotions2int))\n",
    "val_labels = to_categorical(val_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(train_data['images'])\n",
    "train_data = np.array(train_data)\n",
    "\n",
    "val_data = list(val_data['images'])\n",
    "val_data = np.array(val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(19026, 48, 48, 1)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7067, 48, 48, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "val_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Creating Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 48, 48, 64)        640       \n",
      "                                                                 \n",
      " batch_normalization (Batch  (None, 48, 48, 64)        256       \n",
      " Normalization)                                                  \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 48, 48, 64)        36928     \n",
      "                                                                 \n",
      " batch_normalization_1 (Bat  (None, 48, 48, 64)        256       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2  (None, 24, 24, 64)        0         \n",
      " D)                                                              \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 24, 24, 64)        0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 24, 24, 128)       73856     \n",
      "                                                                 \n",
      " batch_normalization_2 (Bat  (None, 24, 24, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 24, 24, 128)       147584    \n",
      "                                                                 \n",
      " batch_normalization_3 (Bat  (None, 24, 24, 128)       512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPoolin  (None, 12, 12, 128)       0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 12, 12, 128)       0         \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 12, 12, 256)       295168    \n",
      "                                                                 \n",
      " batch_normalization_4 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 12, 12, 256)       590080    \n",
      "                                                                 \n",
      " batch_normalization_5 (Bat  (None, 12, 12, 256)       1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_2 (MaxPoolin  (None, 6, 6, 256)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 6, 6, 256)         0         \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 6, 6, 512)         1180160   \n",
      "                                                                 \n",
      " batch_normalization_6 (Bat  (None, 6, 6, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " conv2d_7 (Conv2D)           (None, 6, 6, 512)         2359808   \n",
      "                                                                 \n",
      " batch_normalization_7 (Bat  (None, 6, 6, 512)         2048      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " max_pooling2d_3 (MaxPoolin  (None, 3, 3, 512)         0         \n",
      " g2D)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 3, 3, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 256)               1179904   \n",
      "                                                                 \n",
      " batch_normalization_8 (Bat  (None, 256)               1024      \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 256)               0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 128)               32896     \n",
      "                                                                 \n",
      " batch_normalization_9 (Bat  (None, 128)               512       \n",
      " chNormalization)                                                \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 128)               0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 64)                8256      \n",
      "                                                                 \n",
      " batch_normalization_10 (Ba  (None, 64)                256       \n",
      " tchNormalization)                                               \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 6)                 390       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5915142 (22.56 MB)\n",
      "Trainable params: 5910406 (22.55 MB)\n",
      "Non-trainable params: 4736 (18.50 KB)\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "\n",
    "# First Block\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(64,(3,3),activation='elu',input_shape=(rows,columns,1),kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Second Block\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(128,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Third Block\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(256,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fourth Block\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Conv2D(512,(3,3),activation='elu',kernel_initializer='he_normal',padding='same'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "\n",
    "# Fifth Block\n",
    "model.add(Flatten())\n",
    "model.add(Dense(256,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Sixth Block\n",
    "model.add(Dense(128,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Seventh Block\n",
    "model.add(Dense(64,activation='elu',kernel_initializer='he_normal'))\n",
    "model.add(BatchNormalization())\n",
    "model.add(Dropout(0.5))\n",
    "\n",
    "# Eighth Block\n",
    "model.add(Dense(classes,activation='softmax',kernel_initializer='he_normal'))\n",
    "\n",
    "print(model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "checkpoint = ModelCheckpoint('/Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5',\n",
    "                             save_best_only=True,\n",
    "                             mode='min',\n",
    "                             monitor='loss',\n",
    "                             verbose=1)\n",
    "\n",
    "earlystopping = EarlyStopping(patience=10,\n",
    "                             verbose=1,\n",
    "                             min_delta=0,\n",
    "                             monitor='accuracy',\n",
    "                             restore_best_weights=True)\n",
    "\n",
    "\n",
    "callbacks = [checkpoint, earlystopping]\n",
    "\n",
    "model.compile(metrics=['accuracy'],\n",
    "             optimizer='rmsprop',\n",
    "             loss='categorical_crossentropy')\n",
    "\n",
    "train_samples = 19026\n",
    "validation_samples = 7067\n",
    "batch_size = 64\n",
    "epochs=25"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Our Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 2.2608 - accuracy: 0.2042WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 110 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 1: loss improved from inf to 2.26075, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 324s 1s/step - loss: 2.2608 - accuracy: 0.2042 - val_loss: 3.5910 - val_accuracy: 0.1486\n",
      "Epoch 2/25\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Library/Frameworks/Python.framework/Versions/3.9/lib/python3.9/site-packages/keras/src/engine/training.py:3079: UserWarning: You are saving your model as an HDF5 file via `model.save()`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')`.\n",
      "  saving_api.save_model(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "297/297 [==============================] - ETA: 0s - loss: 1.6420 - accuracy: 0.3302\n",
      "Epoch 2: loss improved from 2.26075 to 1.64195, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 315s 1s/step - loss: 1.6420 - accuracy: 0.3302\n",
      "Epoch 3/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.3922 - accuracy: 0.4451\n",
      "Epoch 3: loss improved from 1.64195 to 1.39221, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 339s 1s/step - loss: 1.3922 - accuracy: 0.4451\n",
      "Epoch 4/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.2777 - accuracy: 0.5043\n",
      "Epoch 4: loss improved from 1.39221 to 1.27766, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 327s 1s/step - loss: 1.2777 - accuracy: 0.5043\n",
      "Epoch 5/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.1978 - accuracy: 0.5411\n",
      "Epoch 5: loss improved from 1.27766 to 1.19782, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 319s 1s/step - loss: 1.1978 - accuracy: 0.5411\n",
      "Epoch 6/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.1205 - accuracy: 0.5736\n",
      "Epoch 6: loss improved from 1.19782 to 1.12053, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 1.1205 - accuracy: 0.5736\n",
      "Epoch 7/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 1.0474 - accuracy: 0.6093\n",
      "Epoch 7: loss improved from 1.12053 to 1.04736, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 319s 1s/step - loss: 1.0474 - accuracy: 0.6093\n",
      "Epoch 8/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.9740 - accuracy: 0.6451\n",
      "Epoch 8: loss improved from 1.04736 to 0.97398, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 0.9740 - accuracy: 0.6451\n",
      "Epoch 9/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.8748 - accuracy: 0.6854\n",
      "Epoch 9: loss improved from 0.97398 to 0.87481, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 324s 1s/step - loss: 0.8748 - accuracy: 0.6854\n",
      "Epoch 10/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7932 - accuracy: 0.7170\n",
      "Epoch 10: loss improved from 0.87481 to 0.79323, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 321s 1s/step - loss: 0.7932 - accuracy: 0.7170\n",
      "Epoch 11/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.7028 - accuracy: 0.7584\n",
      "Epoch 11: loss improved from 0.79323 to 0.70277, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 321s 1s/step - loss: 0.7028 - accuracy: 0.7584\n",
      "Epoch 12/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.6015 - accuracy: 0.7966\n",
      "Epoch 12: loss improved from 0.70277 to 0.60149, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 321s 1s/step - loss: 0.6015 - accuracy: 0.7966\n",
      "Epoch 13/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.5243 - accuracy: 0.8256\n",
      "Epoch 13: loss improved from 0.60149 to 0.52433, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 322s 1s/step - loss: 0.5243 - accuracy: 0.8256\n",
      "Epoch 14/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.4513 - accuracy: 0.8511\n",
      "Epoch 14: loss improved from 0.52433 to 0.45134, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 0.4513 - accuracy: 0.8511\n",
      "Epoch 15/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.3961 - accuracy: 0.8724\n",
      "Epoch 15: loss improved from 0.45134 to 0.39610, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 324s 1s/step - loss: 0.3961 - accuracy: 0.8724\n",
      "Epoch 16/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.3476 - accuracy: 0.8885\n",
      "Epoch 16: loss improved from 0.39610 to 0.34755, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 325s 1s/step - loss: 0.3476 - accuracy: 0.8885\n",
      "Epoch 17/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2929 - accuracy: 0.9067\n",
      "Epoch 17: loss improved from 0.34755 to 0.29287, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 326s 1s/step - loss: 0.2929 - accuracy: 0.9067\n",
      "Epoch 18/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2808 - accuracy: 0.9110\n",
      "Epoch 18: loss improved from 0.29287 to 0.28083, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 0.2808 - accuracy: 0.9110\n",
      "Epoch 19/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2436 - accuracy: 0.9236\n",
      "Epoch 19: loss improved from 0.28083 to 0.24363, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 322s 1s/step - loss: 0.2436 - accuracy: 0.9236\n",
      "Epoch 20/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2316 - accuracy: 0.9288\n",
      "Epoch 20: loss improved from 0.24363 to 0.23159, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 0.2316 - accuracy: 0.9288\n",
      "Epoch 21/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2102 - accuracy: 0.9358\n",
      "Epoch 21: loss improved from 0.23159 to 0.21023, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 323s 1s/step - loss: 0.2102 - accuracy: 0.9358\n",
      "Epoch 22/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.2059 - accuracy: 0.9394\n",
      "Epoch 22: loss improved from 0.21023 to 0.20586, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 325s 1s/step - loss: 0.2059 - accuracy: 0.9394\n",
      "Epoch 23/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.1869 - accuracy: 0.9437\n",
      "Epoch 23: loss improved from 0.20586 to 0.18690, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 322s 1s/step - loss: 0.1869 - accuracy: 0.9437\n",
      "Epoch 24/25\n",
      "297/297 [==============================] - ETA: 0s - loss: 0.1781 - accuracy: 0.9456\n",
      "Epoch 24: loss improved from 0.18690 to 0.17812, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 328s 1s/step - loss: 0.1781 - accuracy: 0.9456\n",
      "Epoch 25/25\n",
      "197/297 [==================>...........] - ETA: 1:50 - loss: 0.1734 - accuracy: 0.9447WARNING:tensorflow:Your input ran out of data; interrupting training. Make sure that your dataset or generator can generate at least `steps_per_epoch * epochs` batches (in this case, 7425 batches). You may need to use the repeat() function when building your dataset.\n",
      "\n",
      "Epoch 25: loss improved from 0.17812 to 0.17339, saving model to /Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5\n",
      "297/297 [==============================] - 217s 730ms/step - loss: 0.1734 - accuracy: 0.9447\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(train_data,\n",
    "                    train_labels,\n",
    "                    epochs=epochs,\n",
    "                    steps_per_epoch=train_samples//batch_size,\n",
    "                    validation_data=(val_data,val_labels),\n",
    "                    validation_steps=validation_samples//batch_size,\n",
    "                    callbacks=callbacks)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Live Predictor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/sy/lscctv1141q8y4mwbvmwrpt80000gn/T/ipykernel_6121/2875727576.py:9: DeprecationWarning: elementwise comparison failed; this will raise an error in the future.\n",
      "  if faces==():\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/1 [==============================] - 0s 178ms/step\n",
      "1/1 [==============================] - 0s 23ms/step\n",
      "1/1 [==============================] - 0s 56ms/step\n",
      "1/1 [==============================] - 0s 43ms/step\n",
      "1/1 [==============================] - 0s 31ms/step\n",
      "1/1 [==============================] - 0s 28ms/step\n",
      "1/1 [==============================] - 0s 33ms/step\n",
      "1/1 [==============================] - 0s 35ms/step\n",
      "1/1 [==============================] - 0s 34ms/step\n",
      "1/1 [==============================] - 0s 29ms/step\n",
      "1/1 [==============================] - 0s 26ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 27ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 21ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 22ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n",
      "1/1 [==============================] - 0s 19ms/step\n",
      "1/1 [==============================] - 0s 20ms/step\n"
     ]
    }
   ],
   "source": [
    "model = load_model('/Users/ashishchauhan/Desktop/Emotion_detection_project/model/class_emotion_detector_V2.h5')\n",
    "cv2.startWindowThread()\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "classifier = cv2.CascadeClassifier('/Users/ashishchauhan/Desktop/Emotion_detection_project/live/haarcascade_frontalface_default.xml')\n",
    "\n",
    "def detect_face(frame):\n",
    "    faces=classifier.detectMultiScale(frame,1.3,4)\n",
    "    if faces==():\n",
    "        return frame\n",
    "    for x,y,w,h in faces:\n",
    "        cv2.rectangle(frame,(x,y),(x+w,y+h),(172,42,251),2)\n",
    "        face = frame[y:y+h,x:x+w]\n",
    "        face = cv2.cvtColor(face,cv2.COLOR_BGR2GRAY)\n",
    "        face = cv2.resize(face,(48,48))\n",
    "        face = face.reshape(1,48,48,1)\n",
    "        cv2.putText(frame,text=int2emotions[np.argmax(model.predict(face))],\n",
    "                    org=(x,y-15),fontFace=cv2.FONT_HERSHEY_SIMPLEX,fontScale=1,color=(106,40,243),thickness=2)\n",
    "    return frame\n",
    "\n",
    "while 1:\n",
    "    ret,frame= cap.read()\n",
    "    if ret==True:\n",
    "        cv2.imshow('emotion_detector',detect_face(frame))\n",
    "        if cv2.waitKey(1)==27:\n",
    "            break\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
